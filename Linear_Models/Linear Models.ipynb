{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames,LinearAlgebra, Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using min-max feature scaling.\n",
    "$$Normalize(\\vec{X}) = \\frac{\\vec{X} -\\vec{X}_{min}}{\\vec{X}_{max}-\\vec{X}_{min}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Normalize(X)=(X .- findmin(X,dims=1)[1]) ./ (findmax(X,dims=1)[1] - findmin(X, dims=1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use root mean-squared error as our primary performance metric.\n",
    "\n",
    "$$RMSE(\\vec{y},\\hat{y}) = \\sqrt{\\frac{1}{n}\\sum{y_i-\\hat{y}_i}}$$\n",
    "\n",
    "where $\\vec{y}$ is a vector of actual values, and $\\hat{y}$ is a vector of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSE (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE(y,ŷ) = sqrt.(sum(ŷ-y).^2/ size(y)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a standard linear model.\n",
    "$$lm(\\vec{X},\\vec{\\beta}) = \\vec{X}^{T}\\vec{\\beta}$$\n",
    "\n",
    "Wher $\\vec{X}$ is a matrix of observations, and $\\vec{\\beta}$ is a vector of coefficients. Note that, if you wish there to be a bias term, it must be concatenated to $\\vec{X}$ as a column of 1's prior to using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lm (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(X,β)= X * β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are wanting to minimize the residual sum of squares (RSS) function. Which thankfully, has a closed-form solution!\n",
    "\n",
    "Objective Function: \n",
    "\n",
    "$$\\vec{\\hat{\\beta}}=\\min_{\\vec{\\hat{\\beta}}} L(D, \\vec{\\beta}) =\\min_{\\vec{\\hat{\\beta}}} \\sum_{i=1}^{n}{(\\hat{\\beta} .\\vec{x_i} - y_i)^2}$$\n",
    "$$L(D,\\vec{\\beta})=||X\\vec{\\beta} - Y||^2$$\n",
    "$$=(X\\vec{\\beta}-y)^T(X\\vec{\\beta}-Y)$$\n",
    "$$=Y^TY-Y^TX\\vec{\\beta}-\\vec{\\beta}^TX^TY+\\vec{\\beta}^TX^TX\\vec{\\beta}$$\n",
    "\n",
    "Get gradient w.r.t. $\\vec{\\beta}$\n",
    "\n",
    "$$\\frac{\\partial{L(D,\\vec{\\beta})}}{\\partial{\\vec{\\beta}}} = \\frac{\\partial{(Y^TY-Y^TX\\vec{\\beta}-\\vec{\\beta}^TX^TY+\\vec{\\beta}X^TX\\vec{\\beta}})}{\\partial{\\vec{\\beta}}}$$\n",
    "$$= -2Y^TX+2\\vec{\\beta}^TX^TX$$\n",
    "$$=-2Y^TX+2\\vec{\\beta}+2\\vec{\\beta}^TX^TX$$\n",
    "\n",
    "Set gradient to zero.\n",
    "\n",
    "$$=-2Y^TX+2\\vec{\\beta}^TX^TX=0$$\n",
    "$$Y^TX=\\vec{\\beta}^TX^TX$$\n",
    "$$X^TY=X^TX\\vec{\\beta}$$\n",
    "$$\\vec{\\beta}=(X^TX)^{-1}X^TY$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ols (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols(X,y)= pinv(X' * X) * X' * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ridge regression, we add an L2 regularization term. Regularization terms are meant to prevent overfitting data by penalizing large weights. It works by taking the L2 norm (otherwise known as the euclidean distance) of the weights and adding them to the loss function. We will also have a $\\lambda$ coefficient on the l2-norm, to be able to tune the weight of the penalty on the overall loss.\n",
    "\n",
    "L2 regularization is defined as follows:\n",
    "$$norm_2(\\vec{\\beta})=||\\vec{\\beta}||_2 = \\beta^T\\beta$$\n",
    "\n",
    "The resulting objective functions comes out as follows:\n",
    "$$\\vec{\\hat{\\beta}}=\\min_{\\vec{\\hat{\\beta}}} L(D, \\vec{\\beta}) =\\min_{\\vec{\\hat{\\beta}}} \\sum_{i=1}^{n}{(\\hat{\\beta} .\\vec{x_i} - y_i)^2} + \\lambda |\\beta|$$\n",
    "$$L(D,\\vec{\\beta})=||X\\vec{\\beta} - Y||^2$ + \\lambda||\\beta||_2$$\n",
    "$$=(\\vec{X}\\vec{\\beta}-\\vec{Y})^T(\\vec{X}\\vec{\\beta}-\\vec{Y}) + \\lambda\\beta^T\\beta$$\n",
    "\n",
    "$$=Y^TY-Y^TX\\vec{\\beta}-\\vec{\\beta}^TX^TY+\\vec{\\beta}^TX^TX\\vec{\\beta}+\\lambda\\beta^T\\beta$$\n",
    "\n",
    "Get gradient w.r.t. $\\vec{\\beta}$\n",
    "\n",
    "$$\\frac{\\partial{L(D,\\vec{\\beta})}}{\\partial{\\vec{\\beta}}} = \\frac{\\partial{(Y^TY-Y^TX\\vec{\\beta}-\\vec{\\beta}^TX^TY+\\vec{\\beta}^TX^TX\\vec{\\beta}+\\lambda\\beta^T\\beta)}{\\partial{\\vec{\\beta}}}$$\n",
    "$$= -2Y^TX+2\\vec{\\beta}^TX^TX$$\n",
    "$$=-2Y^TX+2\\vec{\\beta}+2\\vec{\\beta}^TX^TX$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ridge_ols (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_ols(X,y,λ) = pinv(X' * X + λ * I) * X' * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = DataFrame(CSV.File(\"/home/john/Documents/julia_data/winequality-red.csv\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Array(wine[!,names(wine)[1:11]])\n",
    "y=Array(wine[!,\"quality\"])\n",
    "X = Normalize(X)\n",
    "X=hcat(X,ones(size(X)[1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13499589484380095"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE(y,lm(X,ridge_ols(X,y,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.013955839618286e-12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE(y,lm(X,ols(X,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
